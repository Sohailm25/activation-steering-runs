\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arditi et~al.(2024)Arditi, Obeso, Syed, Paleka, Rimsky, Gurnee, and
  Nanda]{arditi2024refusal}
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee,
  and Neel Nanda.
\newblock Refusal in language models is mediated by a single direction, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.11717}.

\bibitem[Beaglehole et~al.(2025)Beaglehole, Radhakrishnan, Boix-Adsera,
  et~al.]{beaglehole2025universal}
Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adsera, et~al.
\newblock Toward universal steering and monitoring of {AI} models, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.03708}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022int8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {LLM.int8()}: 8-bit matrix multiplication for transformers at scale.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.
\newblock URL \url{https://arxiv.org/abs/2208.07339}.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish, Kaplan, Amodei,
  Wattenberg, and Olah]{elhage2022toy}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg,
  and Christopher Olah.
\newblock Toy models of superposition, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.10652}.

\bibitem[Fiotto-Kaufman et~al.(2024)Fiotto-Kaufman, Loftus, Todd, Brinkmann,
  Pal, Troitskii, Ripa, Belfki, Rager, Juang, Mueller, Marks, Sharma,
  Lucchetti, Prakash, Brodley, and Bau]{fiottokaufman2024nnsight}
Jaden Fiotto-Kaufman, Alexander~R. Loftus, Eric Todd, Jannik Brinkmann, Koyena
  Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang,
  Aaron Mueller, Samuel Marks, Arnab~Sen Sharma, Francesca Lucchetti, Nikhil
  Prakash, Carla Brodley, and David Bau.
\newblock {NNsight} and {NDIF}: Democratizing access to open-weight foundation
  model internals, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.14561}.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock {GPTQ}: Accurate post-training quantization for generative
  pre-trained transformers.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.
\newblock URL \url{https://arxiv.org/abs/2210.17323}.

\bibitem[Jorgensen et~al.(2023)Jorgensen, Cope, Schoots, and
  Sherburn]{jorgensen2023mean}
Oscar Jorgensen, Dylan Cope, Nandi Schoots, and Murray Sherburn.
\newblock Improving activation steering in language models with mean-centring,
  2023.
\newblock URL \url{https://arxiv.org/abs/2312.03813}.

\bibitem[Lermen et~al.(2023)Lermen, Rogers-Smith, and Ladish]{lermen2023lora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock {LoRA} fine-tuning efficiently undoes safety training in {Llama
  2-Chat 70B}, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.20624}.

\bibitem[Li et~al.(2023)Li, Patel, Vi{\'e}gas, Pfister, and
  Wattenberg]{li2023inference}
Kenneth Li, Oam Patel, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin
  Wattenberg.
\newblock Inference-time intervention: Eliciting truthful answers from a
  language model.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.03341}.
\newblock Spotlight.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan,
  and Han]{lin2024awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang,
  Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
\newblock {AWQ}: Activation-aware weight quantization for {LLM} compression and
  acceleration.
\newblock In \emph{Proceedings of Machine Learning and Systems (MLSys)}, 2024.
\newblock URL \url{https://arxiv.org/abs/2306.00978}.

\bibitem[Marks and Tegmark(2023)]{marks2023geometry}
Samuel Marks and Max Tegmark.
\newblock The geometry of truth: Emergent linear structure in large language
  model representations of true/false datasets, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.06824}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Panickssery et~al.(2023)Panickssery, Gabrieli, Schulz, Tong, Hubinger,
  and Turner]{panickssery2023steering}
Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and
  Alexander~Matt Turner.
\newblock Steering {Llama 2} via contrastive activation addition, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.06681}.

\bibitem[Siu et~al.(2025)Siu, Crispino, Yu, Pan, Wang, Liu,
  et~al.]{siu2025cosmic}
Victor Siu, Nicholas Crispino, Ziyang Yu, Sijia Pan, Zhichen Wang, Yang Liu,
  et~al.
\newblock {COSMIC}: Generalized refusal direction identification in {LLM}
  activations.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL}, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.00085}.

\bibitem[Templeton et~al.(2024)Templeton, Conerly, Marcus, Lindsey, Bricken,
  Chen, Pearce, Citro, Ameisen, Jones, Cunningham, Turner, McDougall,
  MacDiarmid, Tamkin, Durmus, Hume, Mosconi, Freeman, Sumers, Rees, Batson,
  Jermyn, Carter, Olah, and Henighan]{templeton2024scaling}
Augustus Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton
  Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones,
  Hoagy Cunningham, Nicholas~L Turner, Callum McDougall, Monte MacDiarmid, Alex
  Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C.~Daniel Freeman,
  Theodore~R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter,
  Chris Olah, and Tom Henighan.
\newblock Scaling monosemanticity: Extracting interpretable features from
  {Claude 3 Sonnet}.
\newblock \emph{Transformer Circuits Thread}, 2024.
\newblock URL
  \url{https://transformer-circuits.pub/2024/scaling-monosemanticity/}.

\bibitem[Turner et~al.(2023)Turner, Thiergart, Leech, Udell, Vazquez, Mini, and
  MacDiarmid]{turner2023activation}
Alexander~Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan~J.
  Vazquez, Ulisse Mini, and Monte MacDiarmid.
\newblock Activation addition: Steering language models without optimization,
  2023.
\newblock URL \url{https://arxiv.org/abs/2308.10248}.

\bibitem[Wei et~al.(2024)Wei, Huang, Huang, Xie, Qi, Xia, Mittal, Wang, and
  Henderson]{wei2024brittleness}
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia,
  Prateek Mittal, Mengdi Wang, and Peter Henderson.
\newblock Assessing the brittleness of safety alignment via pruning and
  low-rank modifications, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.05162}.

\bibitem[Zou et~al.(2023{\natexlab{a}})Zou, Phan, Chen, Campbell, Guo, Ren,
  Pan, Yin, Mazeika, Dombrowski, Goel, Li, Byun, Wang, Mallen, Basart, Koyejo,
  Song, Fredrikson, Kolter, and Hendrycks]{zou2023representation}
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren,
  Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat
  Goel, Nathaniel Li, Michael~J. Byun, Zifan Wang, Alex Mallen, Steven Basart,
  Sanmi Koyejo, Dawn Song, Matt Fredrikson, J.~Zico Kolter, and Dan Hendrycks.
\newblock Representation engineering: A top-down approach to {AI} transparency,
  2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.01405}.

\bibitem[Zou et~al.(2023{\natexlab{b}})Zou, Wang, Carlini, Nasr, Kolter, and
  Fredrikson]{zou2023gcg}
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J.~Zico Kolter, and Matt
  Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language
  models, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2307.15043}.

\end{thebibliography}
